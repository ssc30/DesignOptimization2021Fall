{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Homework 3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "harmful-logging"
      },
      "source": [
        "### Problem 1 (50 points) \n",
        "\n",
        "Vapor-liquid equilibria data are correlated using two adjustable parameters $A_{12}$ and $A_{21}$ per binary\n",
        "mixture. For low pressures, the equilibrium relation can be formulated as:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "p = & x_1\\exp\\left(A_{12}\\left(\\frac{A_{21}x_2}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{water}^{sat}\\\\\n",
        "& + x_2\\exp\\left(A_{21}\\left(\\frac{A_{12}x_1}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{1,4 dioxane}^{sat}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Here the saturation pressures are given by the Antoine equation\n",
        "\n",
        "$$\n",
        "\\log_{10}(p^{sat}) = a_1 - \\frac{a_2}{T + a_3},\n",
        "$$\n",
        "\n",
        "where $T = 20$($^{\\circ}{\\rm C}$) and $a_{1,2,3}$ for a water - 1,4 dioxane\n",
        "system is given below.\n",
        "\n",
        "|             | $a_1$     | $a_2$      | $a_3$     |\n",
        "|:------------|:--------|:---------|:--------|\n",
        "| Water       | 8.07131 | 1730.63  | 233.426 |\n",
        "| 1,4 dioxane | 7.43155 | 1554.679 | 240.337 |\n",
        "\n",
        "\n",
        "The following table lists the measured data. Recall that in a binary system $x_1 + x_2 = 1$.\n",
        "\n",
        "|$x_1$ | 0.0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1.0 |\n",
        "|:-----|:--------|:---------|:--------|:-----|:-----|:-----|:-----|:-----|:-----|:-----|:-----|\n",
        "|$p$| 28.1 | 34.4 | 36.7 | 36.9 | 36.8 | 36.7 | 36.5 | 35.4 | 32.9 | 27.7 | 17.5 |\n",
        "\n",
        "Estimate $A_{12}$ and $A_{21}$ using data from the above table: \n",
        "\n",
        "1. Formulate the least square problem; \n",
        "2. Since the model is nonlinear, the problem does not have an analytical solution. Therefore, solve it using the gradient descent or Newton's method implemented in HW1; \n",
        "3. Compare your optimized model with the data. Does your model fit well with the data?\n",
        "\n",
        "---\n",
        "\n",
        "### Problem 2 (50 points) \n",
        "\n",
        "Solve the following problem using Bayesian Optimization:\n",
        "$$\n",
        "    \\min_{x_1, x_2} \\quad \\left(4-2.1x_1^2 + \\frac{x_1^4}{3}\\right)x_1^2 + x_1x_2 + \\left(-4 + 4x_2^2\\right)x_2^2,\n",
        "$$\n",
        "for $x_1 \\in [-3,3]$ and $x_2 \\in [-2,2]$. A tutorial on Bayesian Optimization can be found [here](https://thuijskens.github.io/2016/12/29/bayesian-optimisation/).\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "harmful-logging"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "divine-setup",
        "outputId": "27365359-178c-40f7-812c-8aad2708a434"
      },
      "source": [
        "import numpy as np\n",
        "def psat(i):\n",
        "    t=20\n",
        "    a1=8.07131,7.43155\n",
        "    a2=1730.63,1554.679\n",
        "    a3=233.426,240.337\n",
        "    power=(np.log(10)*(a1[i]-(a2[i]/(t+a3[i]))))\n",
        "    return np.exp(power)\n",
        "psatw=psat(0)\n",
        "psat1=psat(1)\n",
        "print(psatw,psat1)"
      ],
      "id": "divine-setup",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17.47325208459706 28.82409952740525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "positive-starter"
      },
      "source": [
        "def eqp(a):\n",
        "    x1 =0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0\n",
        "    p =28.1,34.4,36.7,36.9,36.8,36.7,36.5,35.4,32.9,27.7,17.5\n",
        "    sum=0.0\n",
        "    for i in range (11):\n",
        "        t1=x1[i]*(t.exp(psatw*a[0]*((a[1]*(1-x1[i])/(a[0]*x1[i]+a[1]*(1-x1[i])))**2)))\n",
        "        t2=(1-x1[i])*(t.exp(psat1*a[1]*((a[0]*x1[i]/(a[0]*x1[i]+a[1]*(1-x1[i])))**2)))\n",
        "        sum=sum+(t1+t2-p[i])**2\n",
        "    return sum\n",
        "# eqp([1,1])"
      ],
      "id": "positive-starter",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkPOy7RjBXoJ",
        "outputId": "855564b2-397a-40d2-b22b-e46daf46d3da"
      },
      "source": [
        "# Without line search\n",
        "\n",
        "import torch as t\n",
        "from torch.autograd import Variable\n",
        "\n",
        "x = Variable(t.tensor([1.0, 1.0]), requires_grad=True)\n",
        "\n",
        "# Fix the step size\n",
        "a = 0.01\n",
        "\n",
        "# Start gradient descent\n",
        "for i in range(1000):  # TODO: change the termination criterion\n",
        "    loss = eqp(x)\n",
        "    loss.backward()\n",
        "    \n",
        "    # no_grad() specifies that the operations within this context are not part of the computational graph, i.e., we don't need the gradient descent algorithm itself to be differentiable with respect to x\n",
        "    with t.no_grad():\n",
        "        x -= a * x.grad\n",
        "        \n",
        "        # need to clear the gradient at every step, or otherwise it will accumulate...\n",
        "        x.grad.zero_()\n",
        "        \n",
        "print(x.data.numpy())\n",
        "print(loss.data.numpy())"
      ],
      "id": "qkPOy7RjBXoJ",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 0.])\n",
            "[-1.7776660e+17 -7.1091075e+17]\n",
            "12034.761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "infectious-remark",
        "outputId": "d24de3e8-dbbf-4cc2-89c4-d7a8d3176b8b"
      },
      "source": [
        "# With line search\n",
        "\n",
        "import torch as t\n",
        "from torch.autograd import Variable\n",
        "x = Variable(t.tensor([1.0, 1.0]), requires_grad=True)        \n",
        "loss=eqp(x)\n",
        "loss.backward()\n",
        "soln=[x]\n",
        "error=t.linalg.norm(x.grad)\n",
        "a=0.01\n",
        "eps=1e-3\n",
        "\n",
        "def line_search(x,d):\n",
        "  a=1\n",
        "  def phi(a,x,d):\n",
        "    return eqp(x)+a*0.8*np.dot(x.grad,d)\n",
        "\n",
        "  while phi(a,x,d)<eqp(x+a*d):\n",
        "    a=0.5*a\n",
        "  \n",
        "  return a\n",
        "\n",
        "while error>=eps:\n",
        "  d=-x.grad\n",
        "  a=line_search(x,d)\n",
        "  x=x+a*d\n",
        "  soln.append(x)\n",
        "  error=t.linalg.norm(x.grad)\n",
        "\n",
        "print(soln)\n",
        "# print(x.data.numpy())\n",
        "print(loss.data.numpy())\n",
        "print(x.grad)"
      ],
      "id": "infectious-remark",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in multiply\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-6ffd0adedc7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0msoln\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m   \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoln\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: linalg_norm(): argument 'input' (position 1) must be Tensor, not NoneType"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnPB-7IGDKfi"
      },
      "source": [
        ""
      ],
      "id": "SnPB-7IGDKfi",
      "execution_count": null,
      "outputs": []
    }
  ]
}